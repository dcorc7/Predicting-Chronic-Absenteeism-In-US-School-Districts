---
title: Methods
---

To predict whether a school district is prone to chronic absenteeism, we employed a variety of statistical learning techniques tailored to capture the complexity and potential non-linearity of influential factors. Selecting the appropriate predictive models is crucial in achieving accurate results because different techniques capture different aspects of the data, especially when relationships between factors are complex and potentially non-linear. Given class imbalance and the public policy implications of false negatives, AUC and recall were prioritized.

To address this challenge, we employed a combination of machine learning and statistical modeling techniques, each offering unique strengths in uncovering relationships among factors influencing chronic absenteeism. These modeling techniques included neural networks, support vector machines (SVM), linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), logistic regression, and random forests. By leveraging a diverse set of models with varying strengths, we ensured that predictions about chronic absenteeism are generalizable and interpretable.

### Neural Networks

Neural networks are flexible, data-driven models inspired by the structure and functioning of neurons in the human brain. These models are composed of interconnected layers of nodes that transform input data through a series of weighted connections and nonlinear activation functions. Each neuron receives input, applies a transformation via an activation function, and passes the result to the next layer. Neural networks can capture complex, non-linear relationships in data that are often missed by traditional linear models.

Neural networks typically have an architecture consisting of an input layer, one or more hidden layers, and an output layer. During training, the neural network learns by adjusting the weights of the connections using optimization algorithms, such as gradient descent. Adjustments in connection weights are dictated by a loss function that measures prediction error. The loss function used in a model depends on the task being performed. Binary classification tasks use binary cross entropy (BCE), multi-class classification tasks use categorical cross entropy (CCE), and regression tasks use mean squared error (MSE). Backpropagation is then applied to compute gradients of the loss concerning the model's weights, allowing the network to learn from mistakes and improve its performance over several training iterations.

#### Binary Classification

To classify districts as either “low absenteeism” or “high absenteeism,” we used a binary classification neural network. The output layer consisted of a single neuron with a sigmoid activation function, yielding a probability between 0 and 1. The model was trained with the binary cross-entropy loss function, and predictions were assigned based on a classification threshold of 0.5.

#### Multi-Class Classification

For categorizing districts into “low,” “medium,” and “high” absenteeism levels, we used a neural network ending in a fully connected layer with three neurons, followed by a softmax activation function to convert the outputs into class probabilities. The model was trained with the categorical cross-entropy loss function, and predictions were made using the `argmax` of the output vector.

Across all tasks, the neural networks were trained using backpropagation and one of the following optimizers based on hyperparameter tuning: Adam, SGD, or RMSProp. Regularization techniques such as dropout and early stopping were employed to reduce overfitting. Hyperparameters including the number of hidden layers, neurons per layer, dropout rate, learning rate, and optimizer type were manually tuned.

### Linear Discriminant Analysis & Quadratic Discriminant Analysis

**Linear Discriminant Analysis (LDA)** is a supervised classification technique that projects high-dimensional data onto a lower-dimensional space to maximize class separability. LDA assumes that all classes are normally distributed and share the same covariance matrix.

The posterior probability of class $k$ given observation $x$ is calculated as:

$$
P(y = k \mid x) = \frac{\pi_k \cdot \mathcal{N}(x \mid \mu_k, \Sigma)}{\sum_j \pi_j \cdot \mathcal{N}(x \mid \mu_j, \Sigma)}
$$

Where:

-   $\pi_k$ is the prior probability of class $k$

-   $\mathcal{N}(x \mid \mu_k, \Sigma)$ is the multivariate normal density

**Quadratic Discriminant Analysis (QDA)** relaxes the assumption of shared covariance matrices. It allows each class to have its covariance matrix, enabling more flexible, nonlinear decision boundaries. However, QDA is more prone to overfitting when sample sizes are small or data is linearly separable.

The QDA posterior probability is:

$$
P(y = k \mid x) = \frac{\pi_k \cdot \mathcal{N}(x \mid \mu_k, \Sigma_k)}{\sum_j \pi_j \cdot \mathcal{N}(x \mid \mu_j, \Sigma_j)}
$$

In this study, six LDA models and three QDA models were fit. Four LDA and two QDA models were trained to predict a binary high absenteeism variable, while the others targeted the multi-class absenteeism levels.

### Logistic Regression

Logistic regression is a statistical method used for binary classification tasks. It models the relationship between a dependent binary variable and one or more independent variables using the logistic function. The logistic function maps any real-valued number into a value between 0 and 1, making it suitable for probability estimation.

In this study, logistic regression was applied to predict whether a school district is prone to chronic absenteeism. The binary target variable, `high_absenteeism`, was coded as 1 if the district's absenteeism rate met or exceeded the average rate (23.6%) and 0 otherwise. A second binary target variable, `high_absenteeism_doe`, instead used an average rate of 28% - a statistic corresponding with the Department of Education's findings.

The logistic regression model was trained using the following equation:

$$
P(y = 1 \mid X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p)}}
$$

Where: - $P(y = 1 \mid X)$ is the probability of the district having an above average chronic absenteeism ratio.

-   $\beta_0$ is the intercept.

-   $\beta_1, \beta_2, \ldots, \beta_p$ are the coefficients for the independent variables $X_1, X_2, \ldots, X_p$.

The model was trained using the maximum likelihood estimation method to find the optimal coefficients. To evaluate its performance, we used metrics such as accuracy, precision, recall, and the area under the ROC curve (AUC).

Two logistic regression models were implemented:

1.  A baseline model using all available predictors.

2.  A refined model using a subset of predictors selected through forward stepwise feature selection based on the Adjusted R-squared value.

### Support Vector Machines

Support Vector Machines (SVM) are supervised learning models for classification and regression tasks. They work by finding the hyperplane that best separates the data into classes while maximizing the margin between classes. SVMs are particularly effective in high-dimensional spaces and are robust to overfitting, especially in cases where the number of dimensions exceeds the number of samples.

#### Binary Classification

For the binary classification task, the SVM model was trained to predict whether a school district is prone to chronic absenteeism (`high_absenteeism` & `high_absenteeism_doe`). A linear kernel, radial kernel, sigmoid kernel, and polynomial kernel were all used to see which one captures relationships within the data best. The model was trained using the following objective:

$$
\begin{aligned}
\max M \\
\text{subject to} \quad 
y_i \left( \sum_{k=1}^{p} w_k x_{ik} + b \right) &\geq M(1 - \xi_i), \quad \forall i \\
\sum_{k=1}^{p} w_k^2 &= 1 \\
\sum_{i=1}^{n} \xi_i &\leq C \\
\xi_i &\geq 0, \quad \forall i
\end{aligned}
$$

Where:

-   $w_k$ are the weights associated with each feature $k$

-   $b$ is the bias term

-   $x_{ik}$ is the value of feature $k$ for observation $i$

-   $y_i$ is the class label for observation $i$

-   $\xi_i$ are slack variables allowing for misclassifications

-   $C$ is the regularization parameter controlling the trade-off between maximizing the margin and allowing classification errors

-   $M$ is the margin to be maximized

The regularization parameter `C` was tuned using a grid search. The model's performance was evaluated using accuracy, AUC, recall, precision, and F1-score.

#### Multi-Class Classification

For the multi-class classification task, the SVM model was extended using the one-vs-all (OvA) approach, where one classifier is trained per class. The previously mentioned kernels were also used for this task. Predictions were made by aggregating the results of all binary classifiers and selecting the class with the highest score.

### Random Forests

Random Forest is an ensemble learning method that builds many decision trees during training and outputs the mode of their predictions for classification tasks (known as majority voting). For binary classification, each tree votes for one of the two classes, and the class with the most votes becomes the model’s final prediction. Random Forests help reduce overfitting compared to a single decision tree by averaging across many trees, leading to improved generalization. The model also naturally provides estimates of feature importance, making it useful not only for prediction but also for understanding which variables are most influential in determining chronic absenteeism.